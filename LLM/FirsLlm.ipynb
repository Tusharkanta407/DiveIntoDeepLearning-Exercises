{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200/1200  loss=0.0000\n",
      "step 400/1200  loss=0.0000\n",
      "step 600/1200  loss=0.0000\n",
      "step 800/1200  loss=0.0000\n",
      "step 1000/1200  loss=0.0000\n",
      "step 1200/1200  loss=0.0000\n",
      "\n",
      "=== Chat with your tiny GPT ===\n",
      "\n",
      "========================\n",
      "PROMPT: pet\n",
      "\n",
      "[Step 1] Context: <BOS> pet <SEP>\n",
      "  Head 0: <BOS>:0.61 | pet:0.27 | <SEP>:0.12\n",
      "  Head 1: <BOS>:0.05 | pet:0.46 | <SEP>:0.50\n",
      "  Head 2: <BOS>:0.13 | pet:0.64 | <SEP>:0.24\n",
      "  Head 3: <BOS>:0.04 | pet:0.16 | <SEP>:0.80\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> pet <SEP> good\n",
      "  Head 0: <BOS>:0.04 | pet:0.16 | <SEP>:0.31 | good:0.50\n",
      "  Head 1: <BOS>:0.02 | pet:0.04 | <SEP>:0.85 | good:0.09\n",
      "  Head 2: <BOS>:0.20 | pet:0.27 | <SEP>:0.14 | good:0.39\n",
      "  Head 3: <BOS>:0.20 | pet:0.09 | <SEP>:0.67 | good:0.04\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: what is my name\n",
      "\n",
      "[Step 1] Context: <BOS> what is my name <SEP>\n",
      "  Head 0: <BOS>:0.32 | what:0.17 | is:0.22 | my:0.14 | name:0.06 | <SEP>:0.10\n",
      "  Head 1: <BOS>:0.01 | what:0.05 | is:0.38 | my:0.08 | name:0.15 | <SEP>:0.32\n",
      "  Head 2: <BOS>:0.11 | what:0.20 | is:0.16 | my:0.19 | name:0.24 | <SEP>:0.11\n",
      "  Head 3: <BOS>:0.02 | what:0.07 | is:0.54 | my:0.06 | name:0.16 | <SEP>:0.15\n",
      "  → Predicted: my\n",
      "\n",
      "[Step 2] Context: <BOS> what is my name <SEP> my\n",
      "  Head 0: <BOS>:0.07 | what:0.18 | is:0.21 | my:0.17 | name:0.10 | <SEP>:0.13 | my:0.13\n",
      "  Head 1: <BOS>:0.01 | what:0.03 | is:0.24 | my:0.22 | name:0.05 | <SEP>:0.40 | my:0.04\n",
      "  Head 2: <BOS>:0.04 | what:0.14 | is:0.06 | my:0.20 | name:0.22 | <SEP>:0.18 | my:0.16\n",
      "  Head 3: <BOS>:0.13 | what:0.05 | is:0.58 | my:0.03 | name:0.08 | <SEP>:0.06 | my:0.07\n",
      "  → Predicted: name\n",
      "\n",
      "[Step 3] Context: <BOS> what is my name <SEP> my name\n",
      "  Head 0: <BOS>:0.13 | what:0.09 | is:0.10 | my:0.26 | name:0.07 | <SEP>:0.09 | my:0.19 | name:0.08\n",
      "  Head 1: <BOS>:0.03 | what:0.05 | is:0.64 | my:0.02 | name:0.08 | <SEP>:0.06 | my:0.03 | name:0.09\n",
      "  Head 2: <BOS>:0.13 | what:0.12 | is:0.12 | my:0.07 | name:0.16 | <SEP>:0.04 | my:0.20 | name:0.15\n",
      "  Head 3: <BOS>:0.02 | what:0.04 | is:0.68 | my:0.01 | name:0.08 | <SEP>:0.04 | my:0.02 | name:0.10\n",
      "  → Predicted: is\n",
      "\n",
      "[Step 4] Context: <BOS> what is my name <SEP> my name is\n",
      "  Head 0: <BOS>:0.39 | what:0.10 | is:0.06 | my:0.08 | name:0.06 | <SEP>:0.04 | my:0.16 | name:0.04 | is:0.06\n",
      "  Head 1: <BOS>:0.09 | what:0.10 | is:0.28 | my:0.05 | name:0.07 | <SEP>:0.10 | my:0.05 | name:0.08 | is:0.17\n",
      "  Head 2: <BOS>:0.12 | what:0.08 | is:0.12 | my:0.10 | name:0.11 | <SEP>:0.09 | my:0.11 | name:0.10 | is:0.18\n",
      "  Head 3: <BOS>:0.11 | what:0.12 | is:0.09 | my:0.13 | name:0.11 | <SEP>:0.06 | my:0.22 | name:0.07 | is:0.09\n",
      "  → Predicted: gpt\n",
      "\n",
      "[Step 5] Context: <BOS> what is my name <SEP> my name is gpt\n",
      "  Head 0: <BOS>:0.50 | what:0.09 | is:0.04 | my:0.05 | name:0.03 | <SEP>:0.02 | my:0.16 | name:0.02 | is:0.04 | gpt:0.06\n",
      "  Head 1: <BOS>:0.06 | what:0.11 | is:0.13 | my:0.16 | name:0.07 | <SEP>:0.12 | my:0.09 | name:0.10 | is:0.11 | gpt:0.05\n",
      "  Head 2: <BOS>:0.04 | what:0.17 | is:0.07 | my:0.12 | name:0.11 | <SEP>:0.05 | my:0.11 | name:0.13 | is:0.10 | gpt:0.10\n",
      "  Head 3: <BOS>:0.07 | what:0.10 | is:0.06 | my:0.17 | name:0.09 | <SEP>:0.06 | my:0.26 | name:0.07 | is:0.07 | gpt:0.06\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: my name is gpt <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n",
      "\n",
      "========================\n",
      "PROMPT: \n",
      "\n",
      "[Step 1] Context: <BOS> <SEP>\n",
      "  Head 0: <BOS>:0.77 | <SEP>:0.23\n",
      "  Head 1: <BOS>:0.05 | <SEP>:0.95\n",
      "  Head 2: <BOS>:0.24 | <SEP>:0.76\n",
      "  Head 3: <BOS>:0.28 | <SEP>:0.72\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 2] Context: <BOS> <SEP> good\n",
      "  Head 0: <BOS>:0.70 | <SEP>:0.12 | good:0.19\n",
      "  Head 1: <BOS>:0.25 | <SEP>:0.43 | good:0.32\n",
      "  Head 2: <BOS>:0.22 | <SEP>:0.42 | good:0.37\n",
      "  Head 3: <BOS>:0.25 | <SEP>:0.27 | good:0.48\n",
      "  → Predicted: good\n",
      "\n",
      "[Step 3] Context: <BOS> <SEP> good good\n",
      "  Head 0: <BOS>:0.03 | <SEP>:0.15 | good:0.26 | good:0.57\n",
      "  Head 1: <BOS>:0.02 | <SEP>:0.12 | good:0.73 | good:0.13\n",
      "  Head 2: <BOS>:0.19 | <SEP>:0.25 | good:0.20 | good:0.36\n",
      "  Head 3: <BOS>:0.11 | <SEP>:0.05 | good:0.82 | good:0.02\n",
      "  → Predicted: <EOS>\n",
      "OUTPUT: good good <EOS>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 241\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Chat with your tiny GPT ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     user_inp = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mEnter input (tokens separated by space, or \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m): \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_inp.lower() == \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    243\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DeepLearning\\deep_learning_env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DeepLearning\\deep_learning_env\\Lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Tiny decoder-only Transformer (GPT-style) that learns multiple (input -> output) pairs\n",
    "# and then generates step-by-step like an LLM, printing attention weights at each step.\n",
    "\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Reproducibility + device\n",
    "# ---------------------------\n",
    "seed = 7\n",
    "random.seed(seed); torch.manual_seed(seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Toy dataset (prompt -> target)\n",
    "# ---------------------------\n",
    "pairs = [\n",
    "    ([\"how\",\"are\",\"you\"], [\"i\",\"am\",\"fine\"]),\n",
    "    ([\"what\",\"is\",\"your\",\"name\"], [\"my\",\"name\",\"is\",\"gpt\"]),\n",
    "    ([\"feeling\"],           [\"good\"]),\n",
    "    ([\"say\",\"animal\"],      [\"cat\"]),\n",
    "    ([\"say\",\"pet\"],         [\"dog\"]),\n",
    "]\n",
    "\n",
    "# Build vocabulary (input + output + special tokens)\n",
    "special = [\"<PAD>\",\"<BOS>\",\"<EOS>\",\"<SEP>\"]\n",
    "tokens = set(special)\n",
    "for x,y in pairs:\n",
    "    tokens.update(x); tokens.update(y)\n",
    "vocab = sorted(tokens, key=lambda t: (t not in special, t))\n",
    "stoi = {t:i for i,t in enumerate(vocab)}\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "PAD, BOS, EOS, SEP = stoi[\"<PAD>\"], stoi[\"<BOS>\"], stoi[\"<EOS>\"], stoi[\"<SEP>\"]\n",
    "vocab_size = len(vocab)\n",
    "# Helpers\n",
    "def encode(tok_list): return [stoi[t] for t in tok_list]\n",
    "def decode(id_list):  return [itos[i] for i in id_list]\n",
    "\n",
    "# Convert (input -> output) pair into training sequence\n",
    "# Format: <BOS> X <SEP> Y <EOS>\n",
    "def build_example(x_tokens, y_tokens):\n",
    "    full = [\"<BOS>\"] + x_tokens + [\"<SEP>\"] + y_tokens + [\"<EOS>\"]\n",
    "    ids = torch.tensor(encode(full), dtype=torch.long)\n",
    "    labels = ids[1:].clone()             # next-token prediction\n",
    "    sep_idx = full.index(\"<SEP>\")\n",
    "    labels[:sep_idx] = -100              # ignore predictions before <SEP>\n",
    "    return ids, labels, sep_idx\n",
    "\n",
    "dataset_raw = [build_example(x,y) for (x,y) in pairs]\n",
    "\n",
    "# Collate batch: pad sequences to equal length\n",
    "def collate(batch):\n",
    "    max_len = max(ids.size(0) for (ids,_,_) in batch)\n",
    "    B = len(batch)\n",
    "    x = torch.full((B, max_len-1), PAD, dtype=torch.long)\n",
    "    y = torch.full((B, max_len-1), -100, dtype=torch.long)\n",
    "    attn_key_padding = torch.zeros((B, max_len-1), dtype=torch.bool)\n",
    "    for i,(ids, labels, _) in enumerate(batch):\n",
    "        Ti = ids.size(0)\n",
    "        x[i, :Ti-1] = ids[:-1]\n",
    "        y[i, :Ti-1] = labels\n",
    "        attn_key_padding[i, :Ti-1] = True\n",
    "    return x, y, attn_key_padding\n",
    "\n",
    "# Training pool (repeat dataset many times to help overfit)\n",
    "train_pool = dataset_raw * 64\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Tiny GPT-like model\n",
    "# ---------------------------\n",
    "\n",
    "# ---- Multi-Head Self-Attention ----\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.h = n_heads\n",
    "        self.d = d_model // n_heads\n",
    "        # Linear projections for Q,K,V\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.o = nn.Linear(d_model, d_model)  # output projection\n",
    "        self.last_attn = None  # store attention weights for inspection\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        # Project into Q, K, V\n",
    "        q = self.q(x).view(B, T, self.h, self.d).transpose(1, 2)  # (B,h,T,d)\n",
    "        k = self.k(x).view(B, T, self.h, self.d).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.h, self.d).transpose(1, 2)\n",
    "        # Scaled dot-product attention\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d)       # (B,h,T,T)\n",
    "        if attn_mask is not None:\n",
    "            att = att.masked_fill(attn_mask == 0, float(\"-inf\"))  # mask future + pad\n",
    "        w = F.softmax(att, dim=-1)                                # attention weights\n",
    "        self.last_attn = w                                        # save for tracing\n",
    "        y = w @ v                                                 # weighted sum of values\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)          # concat heads\n",
    "        return self.o(y)\n",
    "\n",
    "# ---- Feed-Forward MLP ----\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "# ---- Transformer Block ----\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadSelfAttention(d_model, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = MLP(d_model, d_ff)\n",
    "    def forward(self, x, attn_mask):\n",
    "        # Attention + residual\n",
    "        x = x + self.attn(self.ln1(x), attn_mask)\n",
    "        # MLP + residual\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# ---- Full Tiny GPT Model ----\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab, d_model=128, n_layers=4, n_heads=4, d_ff=256, max_len=64, pad_id=0):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        # Embedding for tokens + positions\n",
    "        self.tok = nn.Embedding(vocab, d_model)\n",
    "        self.pos = nn.Embedding(max_len, d_model)\n",
    "        # Stack of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)    # final normalization\n",
    "        self.head = nn.Linear(d_model, vocab, bias=False)  # vocab prediction head\n",
    "        # Tie weights with embedding\n",
    "        self.head.weight = self.tok.weight\n",
    "\n",
    "    # Build causal + padding mask\n",
    "    def _build_attn_mask(self, idx, key_padding_mask):\n",
    "        B, T = idx.shape\n",
    "        causal = torch.tril(torch.ones(T, T, device=idx.device, dtype=torch.bool))  # lower-triangular\n",
    "        kpad = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n",
    "        mask = causal.unsqueeze(0).unsqueeze(0) & kpad     # combine causal + key mask\n",
    "        return mask\n",
    "\n",
    "    def forward(self, idx, key_padding_mask):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
    "        # Embedding lookup\n",
    "        x = self.tok(idx) + self.pos(pos)\n",
    "        attn_mask = self._build_attn_mask(idx, key_padding_mask)\n",
    "        # Pass through blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, attn_mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # vocab prediction\n",
    "        return logits\n",
    "\n",
    "    # Generate tokens step by step with attention tracing\n",
    "    @torch.no_grad()\n",
    "    def generate_with_trace(self, prefix_ids, max_new_tokens=8):\n",
    "        self.eval()\n",
    "        out = prefix_ids.clone()\n",
    "        while out.size(1) < prefix_ids.size(1) + max_new_tokens:\n",
    "            kpad = torch.ones_like(out, dtype=torch.bool, device=out.device)\n",
    "            logits = self.forward(out, kpad)             # forward pass\n",
    "            next_logits = logits[:, -1, :]\n",
    "            next_id = torch.argmax(next_logits, dim=-1, keepdim=True)  # greedy choice\n",
    "            # Trace attention from last block\n",
    "            last_block = self.blocks[-1]\n",
    "            attn = last_block.attn.last_attn  # (1,h,T,T)\n",
    "            # Print attention context\n",
    "            toks = decode(out[0].tolist())\n",
    "            print(f\"\\n[Step {out.size(1)-prefix_ids.size(1)+1}] Context: {' '.join(toks)}\")\n",
    "            for h in range(attn.size(1)):\n",
    "                weights = attn[0, h, -1, :out.size(1)].tolist()\n",
    "                pairs = [f\"{tok}:{w:.2f}\" for tok, w in zip(toks, weights)]\n",
    "                print(f\"  Head {h}: \" + \" | \".join(pairs))\n",
    "            # Append predicted token\n",
    "            out = torch.cat([out, next_id], dim=1)\n",
    "            print(f\"  → Predicted: {decode([next_id.item()])[0]}\")\n",
    "            if next_id.item() == EOS:\n",
    "                break\n",
    "        return out\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Training loop\n",
    "# ---------------------------\n",
    "model = TinyGPT(vocab_size, d_model=128, n_layers=4, n_heads=4, d_ff=256, max_len=64, pad_id=PAD).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "def sample_batch(batch_size=8):\n",
    "    batch = random.sample(train_pool, batch_size)\n",
    "    x, y, kpad = collate(batch)\n",
    "    return x.to(device), y.to(device), kpad.to(device)\n",
    "\n",
    "steps = 1200\n",
    "model.train()\n",
    "for step in range(1, steps+1):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    x, y, kpad = sample_batch(16)\n",
    "    logits = model(x, kpad)\n",
    "    loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if step % 200 == 0:\n",
    "        print(f\"step {step}/{steps}  loss={loss.item():.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Inference with step-by-step attention\n",
    "# ---------------------------\n",
    "model.eval()\n",
    "def run_prompt(prompt_tokens, max_new=6):\n",
    "    prefix = [\"<BOS>\"] + prompt_tokens + [\"<SEP>\"]\n",
    "    prefix_ids = torch.tensor([encode(prefix)], dtype=torch.long, device=device)\n",
    "    print(\"\\n========================\")\n",
    "    print(\"PROMPT:\", \" \".join(prompt_tokens))\n",
    "    gen = model.generate_with_trace(prefix_ids, max_new_tokens=max_new)\n",
    "    out_tokens = decode(gen[0].tolist())\n",
    "    if \"<SEP>\" in out_tokens:\n",
    "        start = out_tokens.index(\"<SEP>\") + 1\n",
    "        predicted = out_tokens[start:]\n",
    "    else:\n",
    "        predicted = out_tokens\n",
    "    predicted = [t for t in predicted if t != \"<PAD>\"]\n",
    "    print(\"OUTPUT:\", \" \".join(predicted))\n",
    "\n",
    "# Try different prompts\n",
    "# ---------------------------\n",
    "# Interactive prediction\n",
    "# ---------------------------\n",
    "print(\"\\n=== Chat with your tiny GPT ===\")\n",
    "while True:\n",
    "    user_inp = input(\"\\nEnter input (tokens separated by space, or 'quit'): \")\n",
    "    if user_inp.lower() == \"quit\":\n",
    "        break\n",
    "    tokens = user_inp.strip().split()\n",
    "    run_prompt(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
