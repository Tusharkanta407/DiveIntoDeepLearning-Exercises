{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07a55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# ---- 1) Rotary Positional Embedding ----\n",
    "def build_rope_frequencies(seq_len, head_dim, base=10000):\n",
    "    \"\"\"Precompute RoPE sine and cosine frequencies.\"\"\"\n",
    "    pos = torch.arange(seq_len)\n",
    "    freq = 1.0 / (base ** (torch.arange(0, head_dim, 2) / head_dim))\n",
    "    angles = pos[:, None] * freq[None, :]  # (seq_len, head_dim/2)\n",
    "    return torch.sin(angles), torch.cos(angles)  # each (seq_len, head_dim/2)\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, T, H, D) where D is even\n",
    "    sin, cos: (T, D/2)\n",
    "    \"\"\"\n",
    "    B, T, H, D = x.shape\n",
    "    x1 = x[..., ::2]  # (B, T, H, D/2)\n",
    "    x2 = x[..., 1::2]\n",
    "\n",
    "    # expand sin, cos for broadcasting\n",
    "    sin = sin[None, :, None, :]  # (1, T, 1, D/2)\n",
    "    cos = cos[None, :, None, :]\n",
    "\n",
    "    x1_rot = x1 * cos - x2 * sin\n",
    "    x2_rot = x1 * sin + x2 * cos\n",
    "    return torch.stack([x1_rot, x2_rot], dim=-1).flatten(-2)  # back to (B,T,H,D)\n",
    "\n",
    "# ---- 2) Multi-Head Attention ----\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        self.Wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, sin, cos, mask=None):\n",
    "        B, T, _ = x.shape\n",
    "        q = self.Wq(x).view(B, T, self.n_heads, self.d_head)\n",
    "        k = self.Wk(x).view(B, T, self.n_heads, self.d_head)\n",
    "        v = self.Wv(x).view(B, T, self.n_heads, self.d_head)\n",
    "\n",
    "        # Apply RoPE to Q and K\n",
    "        q = apply_rope(q, sin[:T], cos[:T])\n",
    "        k = apply_rope(k, sin[:T], cos[:T])\n",
    "\n",
    "        # Attention\n",
    "        scores = torch.einsum('bthd,bshd->bhts', q, k) / math.sqrt(self.d_head)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        context = torch.einsum('bhts,bshd->bthd', attn, v)\n",
    "        context = context.reshape(B, T, self.d_model)\n",
    "        return self.Wo(context)\n",
    "\n",
    "# ---- 3) SwiGLU Feed-Forward ----\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(d_model, d_ff)\n",
    "        self.W2 = nn.Linear(d_model, d_ff)\n",
    "        self.W_out = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W_out(F.silu(self.W1(x)) * self.W2(x))\n",
    "\n",
    "# ---- 4) Transformer Block ----\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = SwiGLU(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x, sin, cos, mask=None):\n",
    "        x = x + self.attn(self.ln1(x), sin, cos, mask)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# ---- 5) Full tiny LM ----\n",
    "class TinyTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, d_ff=256, n_layers=2, max_len=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.unembed.weight = self.embed.weight  # weight tying\n",
    "        self.sin, self.cos = build_rope_frequencies(max_len, d_model // n_heads)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        x = self.embed(idx)\n",
    "\n",
    "        # Causal mask (triangular)\n",
    "        mask = torch.tril(torch.ones(T, T, device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, self.sin.to(x.device), self.cos.to(x.device), mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.unembed(x)\n",
    "        return logits\n",
    "\n",
    "# ---- Test ----\n",
    "vocab_size = 100\n",
    "model = TinyTransformerLM(vocab_size)\n",
    "\n",
    "idx = torch.randint(0, vocab_size, (2, 10))  # (batch=2, seq=10)\n",
    "logits = model(idx)\n",
    "print(\"Logits shape:\", logits.shape)  # (2, 10, vocab_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
